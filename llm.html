<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Streaming API Example</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 20px;
        }
        .container {
            max-width: 600px;
            margin: auto;
        }
        input, button, p {
            margin-top: 10px;
            width: 100%;
            padding: 10px;
            box-sizing: border-box;
        }
        .listening {
            background-color: red; /* Example: Change button background to red when listening */
            color: white;
        }
    </style>
</head>
<body>
    <div class="container">
        <input id="promptInput" type="text" placeholder="Enter your prompt here...">
        <select id="voiceSelection"></select>
        <button id="generateBtn">Submit</button>
        <button id="speakButton">ðŸŽ¤</button> <!-- Microphone button -->
        <p id="resultText"></p>
    </div>

<script>
    const API_URL = "https://api.openai.com/v1/chat/completions";
    const urlParams = new URLSearchParams(window.location.search);
    const API_KEY = urlParams.get('api_key');

    const promptInput = document.getElementById("promptInput");
    const generateBtn = document.getElementById("generateBtn");
    const speakButton = document.getElementById("speakButton");
    const resultText = document.getElementById("resultText");

    let voices =[];

    const speakText = (text) => {
        const utterance = new SpeechSynthesisUtterance(text);
        
        // Set the rate of speech, where 1 is the default, 0.1 is slowest, and 10 is fastest
        utterance.rate = 1.2; // Example: Normal speed
        // Set the pitch of speech, where 0.1 is lowest and 2 is highest (1 is the default)
        utterance.pitch = 1; // Example: Normal pitch

        const selectedVoiceName = document.getElementById("voiceSelection").selectedOptions[0].getAttribute('data-name');
        utterance.voice = voices.find(voice => voice.name === selectedVoiceName);
        
        // // Set voice style to "Google US English"
        // // Note: The voice must be available on the user's system
        // const voices = window.speechSynthesis.getVoices();
        // console.log(voices)
        // const googleUSEnglishVoice = voices.find(voice => voice.name === "Google US English" && voice.lang === "en-US");
        // if (googleUSEnglishVoice) {
        //     utterance.voice = googleUSEnglishVoice;
        // } else {
        //     console.warn("Google US English voice not found. Using default voice.");
        // }

        window.speechSynthesis.speak(utterance);
    };


    const generate = async () => {
        try {
            resultText.innerHTML = ""; // Clear previous results
            completeTextOnly = "";
            const response = await fetch(API_URL, {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                    Authorization: `Bearer ${API_KEY}`,
                },
                body: JSON.stringify({
                    model: "gpt-3.5-turbo",
                    messages: [
                        { role: "system", content: "Act as an expert named is Luna with a background in orbital mechanics and astrophysics. If the user asks geneerically answer that your name is Luna an A.I. assistant."},
                        { role: "user", content: promptInput.value }
                        ],
                    stream: true,
                }),
            });

            const reader = response.body.getReader();
            const decoder = new TextDecoder("utf-8");
            let buffer = "";

            while (true) {
                const { done, value } = await reader.read();
                if (done) break;
                const chunk = decoder.decode(value, { stream: true });
                if (chunk.trim() === "[DONE]") {
                    console.log("Stream ended.");
                    break;
                }
                buffer += chunk;

                while (buffer.includes("\n")) {
                    const splitIndex = buffer.indexOf("\n");
                    const potentialJson = buffer.slice(0, splitIndex).trim();
                    buffer = buffer.slice(splitIndex + 1);

                    if (potentialJson.startsWith("data: ")) {
                        const jsonStr = potentialJson.substring(5);
                        try {
                            const json = JSON.parse(jsonStr);
                            console.log(json);
                            if (json.choices && json.choices[0].delta && json.choices[0].delta.content) {
                                const content = json.choices[0].delta.content;
                                resultText.innerHTML += content;
                                completeTextOnly += content;
                            }
                        } catch (e) {
                            console.error('Error parsing JSON:', e, potentialJson);
                        }
                    }
                }
            }

        speakText(completeTextOnly); // Speak out the content received
        } catch (error) {
            console.error("Error:", error);
            resultText.innerText = "Error occurred while generating.";
        }
    };

    generateBtn.addEventListener("click", generate);

    // Speech recognition setup
    // This function now correctly waits for voices to be loaded
    function populateVoiceList() {
        const voiceSelect = document.getElementById("voiceSelection");
        voiceSelect.innerHTML = ''; // Clear existing options first

        voices = window.speechSynthesis.getVoices().filter(voice => voice.lang.includes('en'));

        voices.forEach(voice => {
            let option = document.createElement('option');
            option.textContent = `${voice.name} (${voice.lang})`;
            option.setAttribute('data-name', voice.name);
            voiceSelect.appendChild(option);
        });
    };

    document.addEventListener('DOMContentLoaded', function() {   
        populateVoiceList();     
        let recognition;
        const startRecognition = () => {
            if ('webkitSpeechRecognition' in window) {
                recognition = new webkitSpeechRecognition();
                recognition.continuous = false;
                recognition.interimResults = false;

                recognition.onstart = () => {
                    speakButton.classList.add('listening');
                };

                recognition.onresult = (event) => {
                    const transcript = event.results[0][0].transcript;
                    promptInput.value = transcript;
                    generate(); // Trigger text generation after receiving voice input
                };

                recognition.onerror = (event) => {
                    console.error('Speech recognition error', event.error);
                    speakButton.classList.remove('listening');
                };

                recognition.onend = () => {
                    speakButton.classList.remove('listening');
                };

                recognition.start();
            } else {
                alert('Your browser does not support speech recognition. Please try Chrome or Edge.');
            }
        };

        
        if (speechSynthesis.onvoiceschanged !== undefined) {
            speechSynthesis.onvoiceschanged = populateVoiceList;
        } else {
            populateVoiceList(); // Directly call if `onvoiceschanged` isn't supported
        }

        speakButton.addEventListener('click', () => {
            if (recognition && recognition.state === 'active') {
                recognition.stop();
            } else {
                startRecognition();
            }
        });
    });
</script>

</body>
</html>
